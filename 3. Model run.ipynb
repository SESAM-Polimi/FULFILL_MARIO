{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b74bae8",
   "metadata": {},
   "source": [
    "# Model run | \"The role of sufficiency measures in a decarbonizing Europe\"\n",
    "\n",
    "This notebook guides you through the process of running all the scenarios for the study\n",
    "\n",
    "> **\"The role of sufficiency measures in a decarbonizing Europe\"**  \n",
    "> Published: 29 April 2025, Ecological Economics  \n",
    "> DOI: [10.1016/j.ecolecon.2025.108645](https://doi.org/10.1016/j.ecolecon.2025.108645)\n",
    "\n",
    "---\n",
    "\n",
    "## About the Zenodo Repository\n",
    "\n",
    "The Zenodo repository contains all data and results related to the study.  \n",
    "**[Zenodo Link](https://zenodo.org/records/15261005)**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Parse the MARIO Database\n",
    "\n",
    "This cell parses the MARIO database. Please run the first two notebooks before this step. Note that you can directly download the database from the Zenodo repository as shown in notebook 2.\n",
    "It uses the `mario` package to load the Supply-Use Table (SUT) in \"flows\" mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ac782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mario\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") # Pandas became a bit noisy with warnings\n",
    "\n",
    "db = mario.parse_from_txt(\n",
    "    path = \"Data/MARIO database/flows\", # it can be also directly downloaded from Zenodo, skipping step 1. Database building\n",
    "    table = 'SUT', \n",
    "    mode ='flows',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e5bc6",
   "metadata": {},
   "source": [
    "### Preparing Shock Calculation\n",
    "\n",
    "The cell below defines the necessary variables and structures for preparing the shock calculation. Key components include:\n",
    "\n",
    "- **Involved Commodities (`inv_commodities`)**: A list of 15 specific commodities involved in the analysis, such as meat products, dairy, and electricity.\n",
    "- **Involved Regions (`inv_regions`)**: A list of 27 European regions included in the study.\n",
    "- **Clusters (`clusters`)**: A dictionary organizing data into categories like `Commodity`, `Activity`, `Region`, and `Consumption category`. Each category contains all available elements and subsets of involved elements.\n",
    "- **Column Definitions**: Lists like `U_cols`, `S_cols`, `Y_cols`, etc., define the structure of various matrices used in the analysis.\n",
    "- **Result Mapping (`Res_map`)**: A dictionary mapping matrix names (e.g., `V`, `F`, `E`) to their column definitions and export names.\n",
    "\n",
    "This setup ensures that the shock calculation process is well-structured and ready for execution in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Preparing shock calculation\n",
    "inv_commodities = ['Products of meat cattle','Products of meat pigs','Products of meat poultry',\n",
    "                   'Dairy products','Fish products','Vegetables; fruit; nuts','Food products nec',\n",
    "                   'products of Vegetable oils and fats','Beverages','Sugar','Liquid fuels',\n",
    "                   'Liquefied Petroleum Gases (LPG)','Natural gas and services related to natural gas extraction; excluding surveying',\n",
    "                   'Chemicals nec','Electricity']\n",
    "\n",
    "inv_regions = ['AT', 'BE', 'BG', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GR', 'HR', 'HU', 'IE', 'LT', 'LU', 'LV', 'MT', 'NL', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK', 'IT']\n",
    "\n",
    "# Defining the clusters\n",
    "clusters = {\n",
    "'Commodity':{\n",
    "    'all':db.get_index('Commodity'),\n",
    "    'Involved commodities': inv_commodities,\n",
    "    },\n",
    "'Activity':{\n",
    "    'all':db.get_index('Activity'),\n",
    "    },\n",
    "'Region':{\n",
    "    'all':db.get_index('Region'),\n",
    "    'Involved regions': inv_regions,\n",
    "    },\n",
    "'Consumption category':{'all':db.get_index('Consumption category')},\n",
    "}\n",
    "\n",
    "# Structuring the export of the results\n",
    "U_cols = ['Region_from','drop','Commodity','Activity', 'drop','Region_to']\n",
    "S_cols = ['Region_from','drop','Activity','Commodity', 'drop','Region_to']\n",
    "Y_cols = ['Region_from','drop','Commodity','Consumption category', 'drop','Region_to']\n",
    "EY_cols = ['Satellite account','Consumption category','drop','Region_to']\n",
    "E_cols = ['Satellite account','Activity','drop','Region_to']\n",
    "F_cols = ['Satellite account','Commodity','drop','Region_to']\n",
    "V_cols = ['Factor of production','Activity','drop','Region_to']\n",
    "X_cols = ['Region_from','drop','Activity']\n",
    "Q_cols = ['Region_from','drop','Commodity']\n",
    "\n",
    "Res_map = {\n",
    "        'V': {'col_names':V_cols, 'exp_name': 'VA'},\n",
    "        'F': {'col_names':F_cols, 'exp_name': 'R_e'},\n",
    "        'e': {'col_names':E_cols, 'exp_name': 'rr'},\n",
    "        'E': {'col_names':E_cols, 'exp_name': 'R'},\n",
    "        'EY': {'col_names':EY_cols, 'exp_name': 'R_hh'},\n",
    "        'f': {'col_names':E_cols, 'exp_name': 'r_ee'},\n",
    "        'Q': {'col_names':Q_cols, 'exp_name': 'Q'},\n",
    "        'X': {'col_names':X_cols, 'exp_name': 'X'}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f7c5e",
   "metadata": {},
   "source": [
    "## 3. Shock Calculation and Results Extraction\n",
    "\n",
    "This section runs the shock calculations and exports the results:\n",
    "\n",
    "- **Scenario Definition**: Scenarios are defined by combinations of background, measure, and year.\n",
    "- **Calculation and Export Options**:  \n",
    "  - `calc`: If `True`, shocks are calculated.  \n",
    "  - `export`: If `True`, results are exported.  \n",
    "  - `forget`: If `True`, each scenario is deleted from memory after export to avoid memory issues.\n",
    "- **Result Aggregation**: Results are aggregated for visualization and saved in the `Outputs/Results` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70032412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Shock calculation and results extraction\n",
    "\n",
    "# Defining the matrices to be exported\n",
    "ex_mat = ['V','F','e','E','EY','f','Q','X']\n",
    "\n",
    "# Define which scenarios to calculate. Scenarios are defined by the combination of background, measure and year.\n",
    "background = [\n",
    "    'REF',  # Reference scenario\n",
    "    'NZE',  # Quicker transition\n",
    "    'BAU'   # Business as usual\n",
    "    ] \n",
    "measure = [\n",
    "    'All',  # All measures\n",
    "    'D',    # Diets\n",
    "    'S',    # Sharing spaces\n",
    "    'M',    # Car efficiency\n",
    "    'P',    # Sharing products\n",
    "    'F',    # Flying less\n",
    "    'B',    # Cycling more\n",
    "    0,      # No measure\n",
    "    ] \n",
    "years = [\n",
    "    '2011',\n",
    "    '2020',\n",
    "    '2025',\n",
    "    '2030',\n",
    "    '2035',\n",
    "    '2040',\n",
    "    '2045',\n",
    "    '2050',\n",
    "    ] # 5 year steps from 2020 to 2050 (pass list of strings)\n",
    "\n",
    "\n",
    "calc = True  # If True,the shocks are calculated. If False, it assumes the calculation are already done and that only export is needed.\n",
    "export = True # If True, the results are exported. If False, it assumes the export is not needed.\n",
    "forget = True # If True, each scenario is deleted after being calculated and exported. This option avoid memory issues in case many scenarios (i.e. more than 10) are calculated. If False, the scenarios are kept in memory\n",
    "\n",
    "res_agg = pd.read_excel(\"Outputs/sets.xlsx\", sheet_name=None) # Da finire per aggregare\n",
    "\n",
    "for y in years:\n",
    "    for b in background:\n",
    "        for m in measure:\n",
    "            \n",
    "            start = time.time()\n",
    "\n",
    "            if calc:\n",
    "                if forget:\n",
    "                    scemario = 'SCEMARIO'\n",
    "                else:\n",
    "                    scemario = f'{b}_{m}_{y}'\n",
    "\n",
    "                db.shock_calc(\n",
    "                f\"Shocks/filled_files/{b}_{m}_{y}.xlsx\",\n",
    "                z=True,\n",
    "                Y=True,\n",
    "                e=True,\n",
    "                scenario=scemario,\n",
    "                force_rewrite=True,\n",
    "                **clusters,\n",
    "                )\n",
    "\n",
    "                end = time.time()\n",
    "                print(f\"Scemario {b}_{m}_{y} calculated in {round(end-start, 2)} seconds.\")\n",
    "\n",
    "\n",
    "            if export:\n",
    "                for mat in ex_mat:\n",
    "                    if mat==ex_mat[-1]:    \n",
    "                        print(mat)              \n",
    "                    else:\n",
    "                        print(mat,end=\" \")              \n",
    "\n",
    "                    if not os.path.exists(f\"Outputs/Results/{Res_map[mat]['exp_name']}\"):\n",
    "                        os.makedirs(f\"Outputs/Results/{Res_map[mat]['exp_name']}\")\n",
    "                    if mat not in ['X','Q']:\n",
    "                        data = db.query(matrices=[mat], scenarios=[scemario]).stack().stack().stack()\n",
    "                    else:\n",
    "                        if mat == 'X':\n",
    "                            data = db.query(matrices=['X'], scenarios=[scemario]).loc[(slice(None),\"Activity\",slice(None)),:]\n",
    "                        if mat == 'Q':\n",
    "                            data = db.query(matrices=['X'], scenarios=[scemario]).loc[(slice(None),\"Commodity\",slice(None)),:]\n",
    "\n",
    "                    data.index.names = Res_map[mat]['col_names']\n",
    "                    drop_levels = [i for i, name in enumerate(data.index.names) if name == 'drop']\n",
    "                    data = data.reset_index(level=drop_levels, drop=True)\n",
    "                    if mat not in ['X','Q']:\n",
    "                        data = data.to_frame().reset_index()\n",
    "                        data = data.rename(columns={0: 'Value'})\n",
    "                    else:\n",
    "                        data = data.rename(columns={'production': 'Value'})\n",
    "                                    \n",
    "                    # Aggregating the results for visualization\n",
    "                    if 'Activity' in data.columns:\n",
    "                        cols_dict = dict(zip(res_agg['_set_ACTIVITIES']['Activity name'], res_agg['_set_ACTIVITIES']['Activity_PBI']))\n",
    "                        data['Activity'] = data['Activity'].map(cols_dict)\n",
    "                        data.set_index('Activity', inplace=True, append=True)\n",
    "\n",
    "                    if 'Commodity' in data.columns:\n",
    "                        cols_dict = dict(zip(res_agg['_set_COMMODITIES']['Commodity name'], res_agg['_set_COMMODITIES']['Commodity_PBI']))\n",
    "                        data['Commodity'] = data['Commodity'].map(cols_dict)\n",
    "                        data.set_index('Commodity', inplace=True, append=True)\n",
    "                    \n",
    "                    data.set_index([col for col in data.columns if col != 'Value'], inplace=True, append=True)\n",
    "                    data = data.groupby(level=list(range(data.index.nlevels))).sum().reset_index()\n",
    "\n",
    "                    data['Year'] = y\n",
    "                    data['Scenario'] = f'{b}_{m}'\n",
    "                    if mat not in ['X','Q']:\n",
    "                        data = data.drop('level_0', axis=1)\n",
    "\n",
    "                    mat_file = f\"Outputs/Results/{Res_map[mat]['exp_name']}/{b}_{m}_{y}.txt\"\n",
    "                    data.to_csv(mat_file, index=False, sep='\\t')\n",
    "\n",
    "                    end = time.time()\n",
    "                    print(f\"Scemario {b}_{m}_{y}: {mat} exported in {round(end-start, 2)} seconds.\")\n",
    "\n",
    "\n",
    "print('Finished!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19413b",
   "metadata": {},
   "source": [
    "## 4. Merge Results (Optional)\n",
    "\n",
    "This section merges all exported results for each matrix into a single file for easier analysis and visualization.  \n",
    "Merged files are saved in `Outputs/Results/Merged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the folder exists\n",
    "results_folder = f\"Outputs/Results/Merged\"\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "for mat, names in Res_map.items():\n",
    "\n",
    "    if mat not in ['U','S']:\n",
    "\n",
    "        files = os.listdir(f\"Outputs/Results/{names['exp_name']}\")\n",
    "        # Placeholder code\n",
    "        dataframes = []\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = f\"{paths['Results']}/{Out_Db}/{names['exp_name']}/{file}\"\n",
    "                df = pd.read_csv(file_path, sep='\\t')\n",
    "                dataframes.append(df)\n",
    "\n",
    "        merged_df = pd.concat(dataframes, axis=0)\n",
    "        merged_df.to_csv(f\"{results_folder}/{names['exp_name']}.txt\", sep='\\t', index=False)\n",
    "        print(f'{mat} exported ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mario",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
